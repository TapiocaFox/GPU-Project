{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PTX Kernel Execution Time Prediction using GPT-4\n",
        "\n",
        "This notebook uses OpenAI's GPT-4 to analyze PTX (Parallel Thread Execution) assembly code and predict execution time for different GPU architectures.\n",
        "\n",
        "## Features:\n",
        "- PTX assembly code analysis\n",
        "- Multi-GPU execution time prediction (RTX_2080_Ti, TITAN_V, RTX_4070, GTX_TITAN_X)\n",
        "- GPU-specific performance analysis\n",
        "- Batch processing of multiple kernels\n",
        "- Automatic GPU classification from PTX filenames\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install openai pandas matplotlib seaborn numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KERNELS_DIR'] = '/content/drive/MyDrive/kernels_src'\n",
        "os.environ['OUTPUT_DIR']  = '/content/drive/MyDrive'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import re\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# Google Colab setup for API key\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"‚úÖ API key loaded from Google Colab secrets\")\n",
        "except ImportError:\n",
        "    api_key = os.getenv('OPENAI_API_KEY')\n",
        "    if not api_key:\n",
        "        import getpass\n",
        "        api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "    print(\"‚úÖ API key loaded from environment/manual input\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading API key: {e}\")\n",
        "    api_key = None\n",
        "\n",
        "# Set up OpenAI client\n",
        "if api_key:\n",
        "    client = openai.OpenAI(api_key=api_key)\n",
        "    print(\"üöÄ OpenAI client initialized successfully!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  OpenAI client not initialized - please check your API key setup\")\n",
        "    client = None\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Architecture Specifications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GPU_SPECS = {\n",
        "    'RTX_2080_Ti': {\n",
        "        'architecture': 'Turing',\n",
        "        'sm_count': 68,\n",
        "        'cuda_cores': 4352,\n",
        "        'tensor_cores': 544,\n",
        "        'base_clock_mhz': 1350,\n",
        "        'boost_clock_mhz': 1545,\n",
        "        'memory_gb': 11,\n",
        "        'memory_bandwidth_gbps': 616,\n",
        "        'l2_cache_mb': 5.5,\n",
        "        'compute_capability': '7.5',\n",
        "        'max_threads_per_block': 1024,\n",
        "        'max_threads_per_sm': 1024,\n",
        "        'warp_size': 32\n",
        "    },\n",
        "    'TITAN_V': {\n",
        "        'architecture': 'Volta',\n",
        "        'sm_count': 80,\n",
        "        'cuda_cores': 5120,\n",
        "        'tensor_cores': 640,\n",
        "        'base_clock_mhz': 1200,\n",
        "        'boost_clock_mhz': 1455,\n",
        "        'memory_gb': 12,\n",
        "        'memory_bandwidth_gbps': 653,\n",
        "        'l2_cache_mb': 4.5,\n",
        "        'compute_capability': '7.0',\n",
        "        'max_threads_per_block': 1024,\n",
        "        'max_threads_per_sm': 2048,\n",
        "        'warp_size': 32\n",
        "    },\n",
        "    'RTX_4070': {\n",
        "        'architecture': 'Ada Lovelace',\n",
        "        'sm_count': 46,\n",
        "        'cuda_cores': 5888,\n",
        "        'tensor_cores': 184,\n",
        "        'base_clock_mhz': 1920,\n",
        "        'boost_clock_mhz': 2475,\n",
        "        'memory_gb': 12,\n",
        "        'memory_bandwidth_gbps': 504,\n",
        "        'l2_cache_mb': 36,\n",
        "        'compute_capability': '8.9',\n",
        "        'max_threads_per_block': 1024,\n",
        "        'max_threads_per_sm': 2048,\n",
        "        'warp_size': 32\n",
        "    },\n",
        "    'GTX_TITAN_X': {\n",
        "        'architecture': 'Maxwell',\n",
        "        'sm_count': 24,\n",
        "        'cuda_cores': 3072,\n",
        "        'base_clock_mhz': 1000,\n",
        "        'boost_clock_mhz': 1075,\n",
        "        'memory_gb': 12,\n",
        "        'memory_bandwidth_gbps': 336,\n",
        "        'l2_cache_mb': 3,\n",
        "        'compute_capability': '5.2',\n",
        "        'max_threads_per_block': 1024,\n",
        "        'max_threads_per_sm': 2048,\n",
        "        'warp_size': 32\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"GPU specifications loaded:\")\n",
        "for gpu, specs in GPU_SPECS.items():\n",
        "    print(f\"  {gpu}: {specs['architecture']} - {specs['cuda_cores']} cores\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PTX File Processing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_ptx_filename(filename: str) -> Optional[Tuple[str, str, str]]:\n",
        "    \"\"\"\n",
        "    Parse PTX filename to extract folder_id, kernel_id, and GPU model.\n",
        "    Format: kernel_{folder_id}_{kernel_id}_{GPU_MODEL}.ptx\n",
        "    Returns: (folder_id, kernel_id, gpu_model) or None\n",
        "    \"\"\"\n",
        "    name = filename.replace('.ptx', '')\n",
        "    match = re.match(r'kernel_(\\d+)_(\\d+)_(.+)', name)\n",
        "    if match:\n",
        "        folder_id = match.group(1)\n",
        "        kernel_id = match.group(2)\n",
        "        gpu_model = match.group(3)\n",
        "        return (folder_id, kernel_id, gpu_model)\n",
        "    return None\n",
        "\n",
        "def classify_ptx_files(kernels_dir: Path) -> Dict[Tuple[str, str, str], Dict[str, Path]]:\n",
        "    \"\"\"\n",
        "    Scan directory for PTX files and classify them by kernel and GPU.\n",
        "    Returns: {(dataset, folder_id, kernel_id): {gpu_model: ptx_file_path}}\n",
        "    \"\"\"\n",
        "    ptx_files = defaultdict(lambda: {})\n",
        "    \n",
        "    for ptx_file in kernels_dir.rglob('*.ptx'):\n",
        "        filename = ptx_file.name\n",
        "        parsed = parse_ptx_filename(filename)\n",
        "        \n",
        "        if parsed is None:\n",
        "            continue\n",
        "        \n",
        "        folder_id, kernel_id, gpu_model = parsed\n",
        "        \n",
        "        # Determine dataset from path\n",
        "        parts = ptx_file.parts\n",
        "        if 'test' in parts:\n",
        "            dataset = 'test'\n",
        "        elif 'validation' in parts:\n",
        "            dataset = 'validation'\n",
        "        else:\n",
        "            dataset = 'unknown'\n",
        "        \n",
        "        key = (dataset, folder_id, kernel_id)\n",
        "        ptx_files[key][gpu_model] = ptx_file\n",
        "    \n",
        "    print(f\"Found {len(ptx_files)} unique kernels with PTX files\")\n",
        "    \n",
        "    # Count GPUs per kernel\n",
        "    gpu_counts = defaultdict(int)\n",
        "    for key, gpus in ptx_files.items():\n",
        "        gpu_counts[len(gpus)] += 1\n",
        "    \n",
        "    print(f\"GPU distribution per kernel:\")\n",
        "    for count, num_kernels in sorted(gpu_counts.items()):\n",
        "        print(f\"  {count} GPU(s): {num_kernels} kernels\")\n",
        "    \n",
        "    return ptx_files\n",
        "\n",
        "print(\"PTX processing functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPT-4 PTX Analysis Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_ptx_analysis_prompt(ptx_code: str, gpu_model: str, grid_dim: Tuple[int, int, int] = (1, 1, 1),\n",
        "                               block_dim: Tuple[int, int, int] = (256, 1, 1),\n",
        "                               data_size: Optional[int] = None) -> str:\n",
        "    \"\"\"\n",
        "    Create a comprehensive prompt for GPT-4 to analyze PTX assembly code.\n",
        "    \"\"\"\n",
        "    gpu_specs = GPU_SPECS.get(gpu_model, {})\n",
        "    \n",
        "    # Extract PTX version and target from code\n",
        "    ptx_version_match = re.search(r'\\.version\\s+(\\d+\\.\\d+)', ptx_code)\n",
        "    ptx_target_match = re.search(r'\\.target\\s+(.+)', ptx_code)\n",
        "    \n",
        "    ptx_version = ptx_version_match.group(1) if ptx_version_match else 'Unknown'\n",
        "    ptx_target = ptx_target_match.group(1) if ptx_target_match else 'Unknown'\n",
        "    \n",
        "    # Limit PTX code size to avoid token limits (keep first 5000 chars)\n",
        "    ptx_preview = ptx_code[:5000] if len(ptx_code) > 5000 else ptx_code\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "You are an expert CUDA PTX (Parallel Thread Execution) assembly code analyst. Analyze the following PTX assembly code and provide detailed performance predictions for the specified GPU architecture.\n",
        "\n",
        "**Target GPU Specifications:**\n",
        "Model: {gpu_model}\n",
        "Architecture: {gpu_specs.get('architecture', 'Unknown')}\n",
        "SM Count: {gpu_specs.get('sm_count', 'Unknown')}\n",
        "CUDA Cores: {gpu_specs.get('cuda_cores', 'Unknown')}\n",
        "Memory Bandwidth: {gpu_specs.get('memory_bandwidth_gbps', 'Unknown')} GB/s\n",
        "L2 Cache: {gpu_specs.get('l2_cache_mb', 'Unknown')} MB\n",
        "Compute Capability: {gpu_specs.get('compute_capability', 'Unknown')}\n",
        "Max Threads per SM: {gpu_specs.get('max_threads_per_sm', 'Unknown')}\n",
        "\n",
        "**PTX Information:**\n",
        "PTX Version: {ptx_version}\n",
        "PTX Target: {ptx_target}\n",
        "\n",
        "**Launch Configuration:**\n",
        "Grid Dimensions: {grid_dim}\n",
        "Block Dimensions: {block_dim}\n",
        "Total Threads: {grid_dim[0] * grid_dim[1] * grid_dim[2] * block_dim[0] * block_dim[1] * block_dim[2]}\n",
        "{f'Data Size: {data_size} elements' if data_size else ''}\n",
        "\n",
        "**PTX Assembly Code:**\n",
        "```ptx\n",
        "{ptx_preview}\n",
        "```\n",
        "\n",
        "**Please provide a comprehensive analysis in the following JSON format:**\n",
        "\n",
        "{{\n",
        "    \"execution_time_estimate\": {{\n",
        "        \"microseconds_min\": <minimum_estimate>,\n",
        "        \"microseconds_max\": <maximum_estimate>,\n",
        "        \"microseconds_typical\": <typical_estimate>,\n",
        "        \"confidence_level\": \"<high/medium/low>\"\n",
        "    }},\n",
        "    \"performance_analysis\": {{\n",
        "        \"primary_bottleneck\": \"<memory/compute/divergence/occupancy>\",\n",
        "        \"bottleneck_explanation\": \"<detailed_explanation>\",\n",
        "        \"arithmetic_intensity\": <ops_per_byte>,\n",
        "        \"memory_pattern\": \"<coalesced/strided/random/complex>\",\n",
        "        \"branch_divergence_risk\": \"<none/low/medium/high>\",\n",
        "        \"occupancy_estimate\": \"<percentage>\"\n",
        "    }},\n",
        "    \"resource_utilization\": {{\n",
        "        \"registers_per_thread\": <estimate>,\n",
        "        \"shared_memory_per_block_bytes\": <estimate>,\n",
        "        \"global_memory_transactions\": <estimate>,\n",
        "        \"instruction_count_estimate\": <estimate>\n",
        "    }},\n",
        "    \"ptx_analysis\": {{\n",
        "        \"instruction_count\": <count>,\n",
        "        \"memory_instructions\": <count>,\n",
        "        \"compute_instructions\": <count>,\n",
        "        \"synchronization_instructions\": <count>,\n",
        "        \"complexity_score\": <0-100>\n",
        "    }},\n",
        "    \"optimization_suggestions\": [\n",
        "        {{\n",
        "            \"category\": \"<memory/computation/occupancy/algorithm>\",\n",
        "            \"suggestion\": \"<detailed_suggestion>\",\n",
        "            \"expected_improvement\": \"<percentage_or_description>\",\n",
        "            \"difficulty\": \"<easy/medium/hard>\"\n",
        "        }}\n",
        "    ]\n",
        "}}\n",
        "\n",
        "Focus on:\n",
        "1. Analyzing PTX instruction patterns and their performance implications\n",
        "2. Realistic timing estimates based on the specific GPU architecture and PTX instructions\n",
        "3. Identification of performance bottlenecks from assembly-level analysis\n",
        "4. Memory access pattern analysis from PTX load/store instructions\n",
        "5. Occupancy and resource utilization estimation\n",
        "6. Actionable optimization recommendations based on PTX code structure\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def analyze_ptx_with_gpt4(ptx_code: str, gpu_model: str, grid_dim: Tuple[int, int, int] = (1, 1, 1),\n",
        "                          block_dim: Tuple[int, int, int] = (256, 1, 1),\n",
        "                          data_size: Optional[int] = None,\n",
        "                          model: str = \"gpt-4o\") -> Dict:\n",
        "    \"\"\"\n",
        "    Analyze PTX assembly code using GPT-4 and return structured performance predictions.\n",
        "    \"\"\"\n",
        "    if client is None:\n",
        "        return {'error': 'OpenAI client not initialized. Please check your API key setup.'}\n",
        "\n",
        "    try:\n",
        "        prompt = create_ptx_analysis_prompt(ptx_code, gpu_model, grid_dim, block_dim, data_size)\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert CUDA PTX assembly code analyst with deep knowledge of GPU architectures, instruction-level performance, and optimization techniques. Provide accurate, detailed analysis in the requested JSON format.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=3000\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content\n",
        "\n",
        "        # Extract JSON from response\n",
        "        json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(0)\n",
        "            analysis = json.loads(json_str)\n",
        "            analysis['_metadata'] = {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'model': model,\n",
        "                'gpu_target': gpu_model,\n",
        "                'grid_dim': grid_dim,\n",
        "                'block_dim': block_dim,\n",
        "                'data_size': data_size,\n",
        "                'raw_response': content\n",
        "            }\n",
        "            return analysis\n",
        "        else:\n",
        "            return {'error': 'Could not parse JSON from response', 'raw_response': content}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "print(\"PTX analysis functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_ptx_files_in_directory(\n",
        "    kernels_dir: Path,\n",
        "    grid_dim: Tuple[int, int, int] = (1, 1, 1),\n",
        "    block_dim: Tuple[int, int, int] = (256, 1, 1),\n",
        "    data_size: Optional[int] = None,\n",
        "    model: str = \"gpt-4o\",\n",
        "    max_kernels: Optional[int] = None\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Analyze all PTX files in the given directory.\n",
        "    \"\"\"\n",
        "    # Classify PTX files\n",
        "    ptx_files = classify_ptx_files(kernels_dir)\n",
        "    \n",
        "    results = {\n",
        "        \"_batch_metadata\": {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"directory\": str(kernels_dir.resolve()),\n",
        "            \"grid_dim\": grid_dim,\n",
        "            \"block_dim\": block_dim,\n",
        "            \"data_size\": data_size,\n",
        "            \"model\": model\n",
        "        },\n",
        "        \"analyses\": {}\n",
        "    }\n",
        "    \n",
        "    # Process each kernel\n",
        "    kernel_keys = list(ptx_files.keys())\n",
        "    if max_kernels:\n",
        "        kernel_keys = kernel_keys[:max_kernels]\n",
        "    \n",
        "    for key_idx, (dataset, folder_id, kernel_id) in enumerate(kernel_keys, 1):\n",
        "        gpu_ptx_map = ptx_files[(dataset, folder_id, kernel_id)]\n",
        "        \n",
        "        print(f\"\\n[{key_idx}/{len(kernel_keys)}] Processing kernel: {dataset}/{folder_id}/{kernel_id}\")\n",
        "        print(f\"  Found PTX files for {len(gpu_ptx_map)} GPU(s): {list(gpu_ptx_map.keys())}\")\n",
        "        \n",
        "        kernel_results = {}\n",
        "        \n",
        "        # Analyze each GPU's PTX file\n",
        "        for gpu_model, ptx_path in gpu_ptx_map.items():\n",
        "            print(f\"    Analyzing {gpu_model} PTX...\")\n",
        "            \n",
        "            try:\n",
        "                # Read PTX file\n",
        "                with open(ptx_path, 'r', errors='ignore') as f:\n",
        "                    ptx_code = f.read()\n",
        "                \n",
        "                # Analyze with GPT-4\n",
        "                analysis = analyze_ptx_with_gpt4(\n",
        "                    ptx_code=ptx_code,\n",
        "                    gpu_model=gpu_model,\n",
        "                    grid_dim=grid_dim,\n",
        "                    block_dim=block_dim,\n",
        "                    data_size=data_size,\n",
        "                    model=model\n",
        "                )\n",
        "                \n",
        "                # Store result with GPU model as key\n",
        "                rel_path = str(ptx_path.relative_to(kernels_dir))\n",
        "                kernel_results[rel_path] = analysis\n",
        "                \n",
        "                if 'error' not in analysis:\n",
        "                    timing = analysis.get('execution_time_estimate', {})\n",
        "                    typical_time = timing.get('microseconds_typical', 'N/A')\n",
        "                    print(f\"      ‚úÖ Predicted time: {typical_time} Œºs\")\n",
        "                else:\n",
        "                    print(f\"      ‚ùå Error: {analysis.get('error', 'Unknown error')}\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"      ‚ùå Exception: {e}\")\n",
        "                rel_path = str(ptx_path.relative_to(kernels_dir))\n",
        "                kernel_results[rel_path] = {'error': str(e)}\n",
        "        \n",
        "        # Store all GPU analyses for this kernel\n",
        "        kernel_key = f\"{dataset}/{folder_id}/{kernel_id}\"\n",
        "        results[\"analyses\"][kernel_key] = kernel_results\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"Batch analysis function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration and Execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths and configuration\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def get_config_paths():\n",
        "    \"\"\"\n",
        "    Resolve important paths from environment variables with sensible defaults.\n",
        "    \"\"\"\n",
        "    kernels_dir = Path(os.getenv(\"KERNELS_DIR\", \"/Users/james/GPU-Project/src/kernels_src\"))\n",
        "    output_dir = Path(os.getenv(\"OUTPUT_DIR\", \"/Users/james/GPU-Project/src/LLM_baseline\"))\n",
        "\n",
        "    kd_exists = kernels_dir.exists()\n",
        "    od_exists = output_dir.exists()\n",
        "\n",
        "    print(\"Configuration paths:\")\n",
        "    print(f\"  KERNELS_DIR: {kernels_dir} (exists: {kd_exists})\")\n",
        "    print(f\"  OUTPUT_DIR : {output_dir} (exists: {od_exists})\")\n",
        "\n",
        "    if not kd_exists:\n",
        "        print(\"‚ö†Ô∏è  KERNELS_DIR does not exist. Please create it or set KERNELS_DIR.\")\n",
        "    if not od_exists:\n",
        "        print(\"‚ö†Ô∏è  OUTPUT_DIR does not exist. It will be created if possible.\")\n",
        "\n",
        "    return kernels_dir, output_dir\n",
        "\n",
        "kernels_dir, output_dir = get_config_paths()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure batch analysis parameters\n",
        "batch_grid_dim = (1, 1, 1)\n",
        "batch_block_dim = (256, 1, 1)\n",
        "batch_data_size = None  # optional\n",
        "max_kernels_to_process = None  # Set to a number to limit processing, None for all\n",
        "\n",
        "print(f\"Batch analysis configuration:\")\n",
        "print(f\"  Grid Dimensions: {batch_grid_dim}\")\n",
        "print(f\"  Block Dimensions: {batch_block_dim}\")\n",
        "print(f\"  Data Size: {batch_data_size}\")\n",
        "print(f\"  Max Kernels: {max_kernels_to_process or 'All'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run batch analysis\n",
        "print(\"Starting batch PTX analysis...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "all_results = analyze_ptx_files_in_directory(\n",
        "    kernels_dir=kernels_dir,\n",
        "    grid_dim=batch_grid_dim,\n",
        "    block_dim=batch_block_dim,\n",
        "    data_size=batch_data_size,\n",
        "    model=\"gpt-4o\",\n",
        "    max_kernels=max_kernels_to_process\n",
        ")\n",
        "\n",
        "# Save results\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "output_name = f\"ptx_kernels_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "output_path = output_dir / output_name\n",
        "\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Batch analysis saved to: {output_path}\")\n",
        "print(f\"üìä Analyzed {len(all_results['analyses'])} kernels\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Analysis and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and analyze results\n",
        "def analyze_ptx_results(results_file: Path):\n",
        "    \"\"\"\n",
        "    Load and analyze PTX prediction results.\n",
        "    \"\"\"\n",
        "    with open(results_file, 'r') as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    # Extract predictions by GPU\n",
        "    gpu_predictions = defaultdict(list)\n",
        "    \n",
        "    for kernel_key, kernel_analyses in results['analyses'].items():\n",
        "        for ptx_path, analysis in kernel_analyses.items():\n",
        "            if 'error' not in analysis:\n",
        "                # Extract GPU from path\n",
        "                gpu_match = re.search(r'_(RTX_2080_Ti|TITAN_V|RTX_4070|GTX_TITAN_X)\\.ptx', ptx_path)\n",
        "                if gpu_match:\n",
        "                    gpu_model = gpu_match.group(1)\n",
        "                    timing = analysis.get('execution_time_estimate', {})\n",
        "                    typical_time = timing.get('microseconds_typical')\n",
        "                    if typical_time:\n",
        "                        gpu_predictions[gpu_model].append(typical_time)\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Box plot by GPU\n",
        "    ax1 = axes[0]\n",
        "    gpu_data = [gpu_predictions[gpu] for gpu in GPU_SPECS.keys() if gpu in gpu_predictions]\n",
        "    gpu_labels = [gpu for gpu in GPU_SPECS.keys() if gpu in gpu_predictions]\n",
        "    \n",
        "    if gpu_data:\n",
        "        ax1.boxplot(gpu_data, labels=gpu_labels)\n",
        "        ax1.set_ylabel('Predicted Execution Time (Œºs)')\n",
        "        ax1.set_title('PTX Execution Time Predictions by GPU')\n",
        "        ax1.tick_params(axis='x', rotation=45)\n",
        "        ax1.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Histogram\n",
        "    ax2 = axes[1]\n",
        "    all_times = []\n",
        "    for gpu, times in gpu_predictions.items():\n",
        "        all_times.extend(times)\n",
        "    \n",
        "    if all_times:\n",
        "        ax2.hist(all_times, bins=30, alpha=0.7, edgecolor='black')\n",
        "        ax2.set_xlabel('Predicted Execution Time (Œºs)')\n",
        "        ax2.set_ylabel('Frequency')\n",
        "        ax2.set_title('Distribution of PTX Execution Time Predictions')\n",
        "        ax2.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print statistics\n",
        "    print(\"\\nüìä Prediction Statistics by GPU:\")\n",
        "    for gpu in GPU_SPECS.keys():\n",
        "        if gpu in gpu_predictions:\n",
        "            times = gpu_predictions[gpu]\n",
        "            print(f\"\\n  {gpu}:\")\n",
        "            print(f\"    Count: {len(times)}\")\n",
        "            print(f\"    Mean: {np.mean(times):.2f} Œºs\")\n",
        "            print(f\"    Median: {np.median(times):.2f} Œºs\")\n",
        "            print(f\"    Min: {np.min(times):.2f} Œºs\")\n",
        "            print(f\"    Max: {np.max(times):.2f} Œºs\")\n",
        "    \n",
        "    return gpu_predictions\n",
        "\n",
        "# Uncomment to analyze results\n",
        "# analyze_ptx_results(output_path)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
