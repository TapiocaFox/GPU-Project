{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Kernel Execution Time Prediction using GPT-4\n",
    "\n",
    "This notebook uses OpenAI's GPT-4 to analyze CUDA kernel source code and predict execution characteristics including timing estimates, bottlenecks, and optimization suggestions.\n",
    "\n",
    "## Features:\n",
    "- Kernel source code analysis\n",
    "- Execution time prediction\n",
    "- Performance bottleneck identification\n",
    "- Optimization recommendations\n",
    "- Support for various GPU architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Google Colab setup for API key\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    # Get API key from Colab secrets\n",
    "    # To set this up: Go to the key icon in the left sidebar ‚Üí Add new secret ‚Üí Name: 'OPENAI_API_KEY'\n",
    "    api_key = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Google Colab secrets\")\nexcept ImportError:\n",
    "    # Fallback for non-Colab environments\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not api_key:\n",
    "        # Manual input as last resort\n",
    "        import getpass\n",
    "        api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    print(\"‚úÖ API key loaded from environment/manual input\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error loading API key: {e}\")\n",
    "    print(\"Please set up your OpenAI API key in Colab secrets (key icon in sidebar)\")\n",
    "    api_key = None\n",
    "\n",
    "# Set up OpenAI client\n",
    "if api_key:\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    print(\"üöÄ OpenAI client initialized successfully!\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  OpenAI client not initialized - please check your API key setup\")\n",
    "    client = None\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Architecture Specifications\n",
    "\n",
    "Define specifications for common GPU architectures to provide context to GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_SPECS = {\n",
    "    'RTX_2080_Ti': {\n",
    "        'architecture': 'Turing',\n",
    "        'sm_count': 68,\n",
    "        'cuda_cores': 4352,\n",
    "        'tensor_cores': 544,\n",
    "        'base_clock_mhz': 1350,\n",
    "        'boost_clock_mhz': 1545,\n",
    "        'memory_gb': 11,\n",
    "        'memory_bandwidth_gbps': 616,\n",
    "        'l2_cache_mb': 5.5,\n",
    "        'compute_capability': '7.5',\n",
    "        'max_threads_per_block': 1024,\n",
    "        'max_threads_per_sm': 1024,\n",
    "        'warp_size': 32\n",
    "    },\n",
    "    'TITAN_V': {\n",
    "        'architecture': 'Volta',\n",
    "        'sm_count': 80,\n",
    "        'cuda_cores': 5120,\n",
    "        'tensor_cores': 640,\n",
    "        'base_clock_mhz': 1200,\n",
    "        'boost_clock_mhz': 1455,\n",
    "        'memory_gb': 12,\n",
    "        'memory_bandwidth_gbps': 653,\n",
    "        'l2_cache_mb': 4.5,\n",
    "        'compute_capability': '7.0',\n",
    "        'max_threads_per_block': 1024,\n",
    "        'max_threads_per_sm': 2048,\n",
    "        'warp_size': 32\n",
    "    },\n",
    "    'RTX_4090': {\n",
    "        'architecture': 'Ada Lovelace',\n",
    "        'sm_count': 128,\n",
    "        'cuda_cores': 16384,\n",
    "        'tensor_cores': 512,\n",
    "        'base_clock_mhz': 2205,\n",
    "        'boost_clock_mhz': 2520,\n",
    "        'memory_gb': 24,\n",
    "        'memory_bandwidth_gbps': 1008,\n",
    "        'l2_cache_mb': 72,\n",
    "        'compute_capability': '8.9',\n",
    "        'max_threads_per_block': 1024,\n",
    "        'max_threads_per_sm': 2048,\n",
    "        'warp_size': 32\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"GPU specifications loaded:\")\n",
    "for gpu, specs in GPU_SPECS.items():\n",
    "    print(f\"  {gpu}: {specs['architecture']} - {specs['cuda_cores']} cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Verification\n",
    "\n",
    "Run this cell to verify your setup is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup verification\n",
    "def verify_setup():\n",
    "    \"\"\"Verify that the notebook is properly set up.\"\"\"\n",
    "    print(\"üîç Verifying setup...\")\n",
    "    \n",
    "    # Check OpenAI client\n",
    "    if client is None:\n",
    "        print(\"‚ùå OpenAI client not initialized\")\n",
    "        print(\"üìù Please set up your API key:\")\n",
    "        print(\"   1. Click the key icon (üîë) in the left sidebar\")\n",
    "        print(\"   2. Add new secret: OPENAI_API_KEY\")\n",
    "        print(\"   3. Paste your OpenAI API key\")\n",
    "        print(\"   4. Enable notebook access\")\n",
    "        print(\"   5. Restart runtime and re-run setup cells\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"‚úÖ OpenAI client initialized\")\n",
    "    \n",
    "    # Test API connection with a simple call\n",
    "    try:\n",
    "        test_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Use cheaper model for testing\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'API test successful'\"}],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        if \"API test successful\" in test_response.choices[0].message.content:\n",
    "            print(\"‚úÖ OpenAI API connection working\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  OpenAI API responded but with unexpected content\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå OpenAI API connection failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Check GPU specs\n",
    "    print(f\"‚úÖ GPU specifications loaded for {len(GPU_SPECS)} models\")\n",
    "    \n",
    "    # Check required imports\n",
    "    required_modules = ['pandas', 'matplotlib', 'seaborn']\n",
    "    for module in required_modules:\n",
    "        try:\n",
    "            __import__(module)\n",
    "            print(f\"‚úÖ {module} imported successfully\")\n",
    "        except ImportError:\n",
    "            print(f\"‚ùå {module} not found - please install it\")\n",
    "            return False\n",
    "    \n",
    "    print(\"\\nüéâ Setup verification complete! Ready to analyze kernels.\")\n",
    "    return True\n",
    "\n",
    "# Run verification\n",
    "verify_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4 Kernel Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_prompt(kernel_code: str, gpu_model: str, grid_dim: Tuple[int, int, int], \n",
    "                          block_dim: Tuple[int, int, int], data_size: Optional[int] = None) -> str:\n",
    "    \"\"\"\n",
    "    Create a comprehensive prompt for GPT-4 to analyze CUDA kernel performance.\n",
    "    \"\"\"\n",
    "    gpu_specs = GPU_SPECS.get(gpu_model, {})\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert CUDA performance analyst. Analyze the following CUDA kernel and provide detailed performance predictions.\n",
    "\n",
    "**Target GPU Specifications:**\n",
    "Model: {gpu_model}\n",
    "Architecture: {gpu_specs.get('architecture', 'Unknown')}\n",
    "SM Count: {gpu_specs.get('sm_count', 'Unknown')}\n",
    "CUDA Cores: {gpu_specs.get('cuda_cores', 'Unknown')}\n",
    "Memory Bandwidth: {gpu_specs.get('memory_bandwidth_gbps', 'Unknown')} GB/s\n",
    "L2 Cache: {gpu_specs.get('l2_cache_mb', 'Unknown')} MB\n",
    "Compute Capability: {gpu_specs.get('compute_capability', 'Unknown')}\n",
    "Max Threads per SM: {gpu_specs.get('max_threads_per_sm', 'Unknown')}\n",
    "\n",
    "**Launch Configuration:**\n",
    "Grid Dimensions: {grid_dim}\n",
    "Block Dimensions: {block_dim}\n",
    "Total Threads: {grid_dim[0] * grid_dim[1] * grid_dim[2] * block_dim[0] * block_dim[1] * block_dim[2]}\n",
    "{f'Data Size: {data_size} elements' if data_size else ''}\n",
    "\n",
    "**CUDA Kernel Source Code:**\n",
    "```cuda\n",
    "{kernel_code}\n",
    "```\n",
    "\n",
    "**Please provide a comprehensive analysis in the following JSON format:**\n",
    "\n",
    "{{\n",
    "    \"execution_time_estimate\": {{\n",
    "        \"microseconds_min\": <minimum_estimate>,\n",
    "        \"microseconds_max\": <maximum_estimate>,\n",
    "        \"microseconds_typical\": <typical_estimate>,\n",
    "        \"confidence_level\": \"<high/medium/low>\"\n",
    "    }},\n",
    "    \"performance_analysis\": {{\n",
    "        \"primary_bottleneck\": \"<memory/compute/divergence/occupancy>\",\n",
    "        \"bottleneck_explanation\": \"<detailed_explanation>\",\n",
    "        \"arithmetic_intensity\": <ops_per_byte>,\n",
    "        \"memory_pattern\": \"<coalesced/strided/random/complex>\",\n",
    "        \"branch_divergence_risk\": \"<none/low/medium/high>\",\n",
    "        \"occupancy_estimate\": \"<percentage>\"\n",
    "    }},\n",
    "    \"resource_utilization\": {{\n",
    "        \"registers_per_thread\": <estimate>,\n",
    "        \"shared_memory_per_block_bytes\": <estimate>,\n",
    "        \"global_memory_transactions\": <estimate>,\n",
    "        \"instruction_count_estimate\": <estimate>\n",
    "    }},\n",
    "    \"optimization_suggestions\": [\n",
    "        {{\n",
    "            \"category\": \"<memory/computation/occupancy/algorithm>\",\n",
    "            \"suggestion\": \"<detailed_suggestion>\",\n",
    "            \"expected_improvement\": \"<percentage_or_description>\",\n",
    "            \"difficulty\": \"<easy/medium/hard>\"\n",
    "        }}\n",
    "    ],\n",
    "    \"scaling_analysis\": {{\n",
    "        \"scales_with_data_size\": \"<linear/sublinear/superlinear>\",\n",
    "        \"parallel_efficiency\": \"<percentage>\",\n",
    "        \"weak_scaling_behavior\": \"<description>\",\n",
    "        \"strong_scaling_behavior\": \"<description>\"\n",
    "    }},\n",
    "    \"comparison_notes\": \"<additional_insights_and_caveats>\"\n",
    "}}\n",
    "\n",
    "Focus on:\n",
    "1. Realistic timing estimates based on the specific GPU architecture\n",
    "2. Identification of performance bottlenecks\n",
    "3. Memory access pattern analysis\n",
    "4. Occupancy and resource utilization\n",
    "5. Actionable optimization recommendations\n",
    "6. Scaling behavior analysis\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def analyze_kernel_with_gpt4(kernel_code: str, gpu_model: str = 'RTX_2080_Ti', \n",
    "                           grid_dim: Tuple[int, int, int] = (1, 1, 1),\n",
    "                           block_dim: Tuple[int, int, int] = (256, 1, 1),\n",
    "                           data_size: Optional[int] = None,\n",
    "                           model: str = \"gpt-4o\") -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze CUDA kernel using GPT-4 and return structured performance predictions.\n",
    "    \"\"\"\n",
    "    # Check if client is initialized\n",
    "    if client is None:\n",
    "        return {\n",
    "            'error': 'OpenAI client not initialized. Please check your API key setup.',\n",
    "            'setup_instructions': 'Go to the key icon in Colab sidebar ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY ‚Üí Paste your key'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        prompt = create_analysis_prompt(kernel_code, gpu_model, grid_dim, block_dim, data_size)\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert CUDA performance analyst with deep knowledge of GPU architectures and optimization techniques. Provide accurate, detailed analysis in the requested JSON format.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1,  # Low temperature for consistent, factual responses\n",
    "            max_tokens=2500\n",
    "        )\n",
    "        \n",
    "        # Extract JSON from the response\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract JSON from the response\n",
    "        json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(0)\n",
    "            analysis = json.loads(json_str)\n",
    "            analysis['_metadata'] = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'model': model,\n",
    "                'gpu_target': gpu_model,\n",
    "                'grid_dim': grid_dim,\n",
    "                'block_dim': block_dim,\n",
    "                'data_size': data_size,\n",
    "                'raw_response': content\n",
    "            }\n",
    "            return analysis\n",
    "        else:\n",
    "            return {'error': 'Could not parse JSON from response', 'raw_response': content}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "print(\"Analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_timing_estimates(analysis: Dict, title: str = \"Kernel Execution Time Estimates\"):\n",
    "    \"\"\"\n",
    "    Create a visualization of timing estimates with confidence intervals.\n",
    "    \"\"\"\n",
    "    if 'execution_time_estimate' not in analysis:\n",
    "        print(\"No timing estimates found in analysis\")\n",
    "        return\n",
    "    \n",
    "    timing = analysis['execution_time_estimate']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Extract timing values\n",
    "    min_time = timing.get('microseconds_min', 0)\n",
    "    max_time = timing.get('microseconds_max', 0)\n",
    "    typical_time = timing.get('microseconds_typical', 0)\n",
    "    confidence = timing.get('confidence_level', 'unknown')\n",
    "    \n",
    "    # Create bar plot with error bars\n",
    "    plt.bar(['Execution Time'], [typical_time], \n",
    "            yerr=[[typical_time - min_time], [max_time - typical_time]], \n",
    "            capsize=10, color='skyblue', alpha=0.7, \n",
    "            error_kw={'elinewidth': 3, 'capthick': 2})\n",
    "    \n",
    "    plt.ylabel('Time (microseconds)')\n",
    "    plt.title(f\"{title}\\n(Confidence: {confidence})\")\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add text annotations\n",
    "    plt.text(0, typical_time + (max_time - typical_time) * 0.1, \n",
    "             f'Range: {min_time:.1f} - {max_time:.1f} Œºs\\nTypical: {typical_time:.1f} Œºs', \n",
    "             ha='center', va='bottom', fontsize=10, \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_performance_dashboard(analysis: Dict):\n",
    "    \"\"\"\n",
    "    Create a comprehensive performance dashboard.\n",
    "    \"\"\"\n",
    "    if 'error' in analysis:\n",
    "        print(f\"Error in analysis: {analysis['error']}\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('CUDA Kernel Performance Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Timing estimates\n",
    "    ax1 = axes[0, 0]\n",
    "    timing = analysis.get('execution_time_estimate', {})\n",
    "    times = [timing.get('microseconds_min', 0), \n",
    "             timing.get('microseconds_typical', 0), \n",
    "             timing.get('microseconds_max', 0)]\n",
    "    ax1.bar(['Min', 'Typical', 'Max'], times, color=['green', 'blue', 'red'], alpha=0.7)\n",
    "    ax1.set_ylabel('Time (Œºs)')\n",
    "    ax1.set_title('Execution Time Estimates')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Resource utilization\n",
    "    ax2 = axes[0, 1]\n",
    "    resources = analysis.get('resource_utilization', {})\n",
    "    resource_names = ['Registers/Thread', 'Shared Mem (KB)', 'Global Transactions', 'Instructions']\n",
    "    resource_values = [\n",
    "        resources.get('registers_per_thread', 0),\n",
    "        resources.get('shared_memory_per_block_bytes', 0) / 1024,\n",
    "        resources.get('global_memory_transactions', 0),\n",
    "        resources.get('instruction_count_estimate', 0)\n",
    "    ]\n",
    "    \n",
    "    # Normalize values for visualization\n",
    "    max_val = max(resource_values) if resource_values else 1\n",
    "    normalized_values = [v / max_val * 100 for v in resource_values]\n",
    "    \n",
    "    ax2.barh(resource_names, normalized_values, color='orange', alpha=0.7)\n",
    "    ax2.set_xlabel('Normalized Usage (%)')\n",
    "    ax2.set_title('Resource Utilization')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 3. Performance metrics\n",
    "    ax3 = axes[1, 0]\n",
    "    perf = analysis.get('performance_analysis', {})\n",
    "    occupancy = float(re.search(r'\\d+', str(perf.get('occupancy_estimate', '0'))).group()) if perf.get('occupancy_estimate') else 0\n",
    "    \n",
    "    metrics = ['Occupancy (%)', 'Arithmetic Intensity']\n",
    "    values = [occupancy, perf.get('arithmetic_intensity', 0)]\n",
    "    \n",
    "    ax3.bar(metrics, values, color=['purple', 'teal'], alpha=0.7)\n",
    "    ax3.set_ylabel('Value')\n",
    "    ax3.set_title('Performance Metrics')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Optimization potential\n",
    "    ax4 = axes[1, 1]\n",
    "    optimizations = analysis.get('optimization_suggestions', [])\n",
    "    \n",
    "    categories = {}\n",
    "    for opt in optimizations:\n",
    "        cat = opt.get('category', 'other')\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "    \n",
    "    if categories:\n",
    "        ax4.pie(categories.values(), labels=categories.keys(), autopct='%1.1f%%', startangle=90)\n",
    "        ax4.set_title('Optimization Opportunities')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No optimization\\nsuggestions', ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('Optimization Opportunities')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_analysis_summary(analysis: Dict):\n",
    "    \"\"\"\n",
    "    Print a formatted summary of the analysis results.\n",
    "    \"\"\"\n",
    "    if 'error' in analysis:\n",
    "        print(f\"‚ùå Analysis Error: {analysis['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîç CUDA KERNEL PERFORMANCE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Execution time\n",
    "    timing = analysis.get('execution_time_estimate', {})\n",
    "    print(f\"\\n‚è±Ô∏è  EXECUTION TIME ESTIMATE:\")\n",
    "    print(f\"   Range: {timing.get('microseconds_min', 'N/A')} - {timing.get('microseconds_max', 'N/A')} Œºs\")\n",
    "    print(f\"   Typical: {timing.get('microseconds_typical', 'N/A')} Œºs\")\n",
    "    print(f\"   Confidence: {timing.get('confidence_level', 'N/A')}\")\n",
    "    \n",
    "    # Performance analysis\n",
    "    perf = analysis.get('performance_analysis', {})\n",
    "    print(f\"\\nüéØ PERFORMANCE ANALYSIS:\")\n",
    "    print(f\"   Primary Bottleneck: {perf.get('primary_bottleneck', 'N/A')}\")\n",
    "    print(f\"   Memory Pattern: {perf.get('memory_pattern', 'N/A')}\")\n",
    "    print(f\"   Branch Divergence Risk: {perf.get('branch_divergence_risk', 'N/A')}\")\n",
    "    print(f\"   Occupancy: {perf.get('occupancy_estimate', 'N/A')}\")\n",
    "    print(f\"   Arithmetic Intensity: {perf.get('arithmetic_intensity', 'N/A')} ops/byte\")\n",
    "    \n",
    "    # Bottleneck explanation\n",
    "    if perf.get('bottleneck_explanation'):\n",
    "        print(f\"\\nüí° BOTTLENECK EXPLANATION:\")\n",
    "        print(f\"   {perf['bottleneck_explanation']}\")\n",
    "    \n",
    "    # Optimization suggestions\n",
    "    optimizations = analysis.get('optimization_suggestions', [])\n",
    "    if optimizations:\n",
    "        print(f\"\\nüöÄ TOP OPTIMIZATION SUGGESTIONS:\")\n",
    "        for i, opt in enumerate(optimizations[:3], 1):\n",
    "            print(f\"   {i}. [{opt.get('category', 'general').upper()}] {opt.get('suggestion', 'N/A')}\")\n",
    "            print(f\"      Expected Improvement: {opt.get('expected_improvement', 'N/A')}\")\n",
    "            print(f\"      Difficulty: {opt.get('difficulty', 'N/A')}\")\n",
    "            print()\n",
    "    \n",
    "    # Scaling analysis\n",
    "    scaling = analysis.get('scaling_analysis', {})\n",
    "    if scaling:\n",
    "        print(f\"\\nüìà SCALING ANALYSIS:\")\n",
    "        print(f\"   Data Size Scaling: {scaling.get('scales_with_data_size', 'N/A')}\")\n",
    "        print(f\"   Parallel Efficiency: {scaling.get('parallel_efficiency', 'N/A')}\")\n",
    "    \n",
    "    # Additional notes\n",
    "    if analysis.get('comparison_notes'):\n",
    "        print(f\"\\nüìù ADDITIONAL NOTES:\")\n",
    "        print(f\"   {analysis['comparison_notes']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"Visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Here's how to use the kernel analyzer with sample CUDA kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple vector addition kernel\n",
    "vector_add_kernel = \"\"\"\n",
    "__global__ void vectorAdd(float *a, float *b, float *c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Analyze the vector addition kernel\n",
    "print(\"Analyzing Vector Addition Kernel...\")\n",
    "analysis_1 = analyze_kernel_with_gpt4(\n",
    "    kernel_code=vector_add_kernel,\n",
    "    gpu_model='RTX_2080_Ti',\n",
    "    grid_dim=(1024, 1, 1),\n",
    "    block_dim=(256, 1, 1),\n",
    "    data_size=1024*256  # 1M elements\n",
    ")\n",
    "\n",
    "if 'error' not in analysis_1:\n",
    "    print_analysis_summary(analysis_1)\n",
    "    create_performance_dashboard(analysis_1)\n",
    "else:\n",
    "    print(f\"Error: {analysis_1['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Matrix multiplication kernel\n",
    "matmul_kernel = \"\"\"\n",
    "__global__ void matrixMul(float *A, float *B, float *C, int N) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < N && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        for (int k = 0; k < N; k++) {\n",
    "            sum += A[row * N + k] * B[k * N + col];\n",
    "        }\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Analyze the matrix multiplication kernel\n",
    "print(\"Analyzing Matrix Multiplication Kernel...\")\n",
    "analysis_2 = analyze_kernel_with_gpt4(\n",
    "    kernel_code=matmul_kernel,\n",
    "    gpu_model='RTX_2080_Ti',\n",
    "    grid_dim=(32, 32, 1),  # 1024x1024 matrix\n",
    "    block_dim=(16, 16, 1),\n",
    "    data_size=1024*1024\n",
    ")\n",
    "\n",
    "if 'error' not in analysis_2:\n",
    "    print_analysis_summary(analysis_2)\n",
    "    create_performance_dashboard(analysis_2)\n",
    "else:\n",
    "    print(f\"Error: {analysis_2['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Kernel Analysis\n",
    "\n",
    "Use this cell to analyze your own kernel code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASTE YOUR KERNEL CODE HERE\n",
    "your_kernel_code = \"\"\"\n",
    "// Replace this with your actual CUDA kernel code\n",
    "__global__ void your_kernel() {\n",
    "    // Your kernel implementation\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Configure analysis parameters\n",
    "target_gpu = 'RTX_2080_Ti'  # Options: 'RTX_2080_Ti', 'TITAN_V', 'RTX_4090'\n",
    "grid_dimensions = (1, 1, 1)  # (gridDim.x, gridDim.y, gridDim.z)\n",
    "block_dimensions = (256, 1, 1)  # (blockDim.x, blockDim.y, blockDim.z)\n",
    "input_data_size = None  # Number of elements (optional)\n",
    "\n",
    "# Run analysis\n",
    "print(f\"Analyzing custom kernel on {target_gpu}...\")\n",
    "custom_analysis = analyze_kernel_with_gpt4(\n",
    "    kernel_code=your_kernel_code,\n",
    "    gpu_model=target_gpu,\n",
    "    grid_dim=grid_dimensions,\n",
    "    block_dim=block_dimensions,\n",
    "    data_size=input_data_size\n",
    ")\n",
    "\n",
    "if 'error' not in custom_analysis:\n",
    "    print_analysis_summary(custom_analysis)\n",
    "    create_performance_dashboard(custom_analysis)\n",
    "    \n",
    "    # Save results to file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"kernel_analysis_{timestamp}.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(custom_analysis, f, indent=2)\n",
    "    print(f\"\\nüíæ Analysis saved to: {filename}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {custom_analysis['error']}\")\n",
    "    if 'raw_response' in custom_analysis:\n",
    "        print(f\"Raw response: {custom_analysis['raw_response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Analysis and Comparison\n",
    "\n",
    "Analyze multiple kernels and compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kernel_analyses(analyses: List[Dict], labels: List[str]):\n",
    "    \"\"\"\n",
    "    Compare multiple kernel analyses side by side.\n",
    "    \"\"\"\n",
    "    if len(analyses) != len(labels):\n",
    "        print(\"Error: Number of analyses must match number of labels\")\n",
    "        return\n",
    "    \n",
    "    # Filter out error analyses\n",
    "    valid_analyses = [(a, l) for a, l in zip(analyses, labels) if 'error' not in a]\n",
    "    \n",
    "    if not valid_analyses:\n",
    "        print(\"No valid analyses to compare\")\n",
    "        return\n",
    "    \n",
    "    analyses, labels = zip(*valid_analyses)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for analysis, label in zip(analyses, labels):\n",
    "        timing = analysis.get('execution_time_estimate', {})\n",
    "        perf = analysis.get('performance_analysis', {})\n",
    "        resources = analysis.get('resource_utilization', {})\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Kernel': label,\n",
    "            'Typical Time (Œºs)': timing.get('microseconds_typical', 'N/A'),\n",
    "            'Primary Bottleneck': perf.get('primary_bottleneck', 'N/A'),\n",
    "            'Memory Pattern': perf.get('memory_pattern', 'N/A'),\n",
    "            'Occupancy': perf.get('occupancy_estimate', 'N/A'),\n",
    "            'Arithmetic Intensity': perf.get('arithmetic_intensity', 'N/A'),\n",
    "            'Registers/Thread': resources.get('registers_per_thread', 'N/A'),\n",
    "            'Shared Memory (KB)': resources.get('shared_memory_per_block_bytes', 0) / 1024 if resources.get('shared_memory_per_block_bytes') else 'N/A'\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\nüìä KERNEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Timing comparison\n",
    "    timing_data = [timing.get('microseconds_typical', 0) for timing in [a.get('execution_time_estimate', {}) for a in analyses]]\n",
    "    axes[0].bar(labels, timing_data, color='skyblue', alpha=0.7)\n",
    "    axes[0].set_ylabel('Execution Time (Œºs)')\n",
    "    axes[0].set_title('Execution Time Comparison')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Occupancy comparison\n",
    "    occupancy_data = []\n",
    "    for analysis in analyses:\n",
    "        perf = analysis.get('performance_analysis', {})\n",
    "        occ_str = str(perf.get('occupancy_estimate', '0'))\n",
    "        occ_match = re.search(r'\\d+', occ_str)\n",
    "        occupancy_data.append(float(occ_match.group()) if occ_match else 0)\n",
    "    \n",
    "    axes[1].bar(labels, occupancy_data, color='orange', alpha=0.7)\n",
    "    axes[1].set_ylabel('Occupancy (%)')\n",
    "    axes[1].set_title('Occupancy Comparison')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: Compare the two kernels we analyzed earlier\n",
    "if 'analysis_1' in locals() and 'analysis_2' in locals():\n",
    "    compare_kernel_analyses(\n",
    "        [analysis_1, analysis_2],\n",
    "        ['Vector Add', 'Matrix Multiply']\n",
    "    )\nelse:\n    print(\"Run the example analyses above first to enable comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "Additional utilities for specialized analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scaling_behavior(kernel_code: str, gpu_model: str, \n",
    "                           data_sizes: List[int], base_grid_dim: Tuple[int, int, int],\n",
    "                           base_block_dim: Tuple[int, int, int]):\n",
    "    \"\"\"\n",
    "    Analyze how kernel performance scales with different data sizes.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for data_size in data_sizes:\n",
    "        # Adjust grid size based on data size\n",
    "        total_threads = base_grid_dim[0] * base_grid_dim[1] * base_grid_dim[2] * \\\n",
    "                       base_block_dim[0] * base_block_dim[1] * base_block_dim[2]\n",
    "        scale_factor = data_size / total_threads\n",
    "        \n",
    "        scaled_grid = (int(base_grid_dim[0] * scale_factor**0.5), \n",
    "                      int(base_grid_dim[1] * scale_factor**0.5), \n",
    "                      base_grid_dim[2])\n",
    "        \n",
    "        print(f\"Analyzing data size: {data_size:,} elements...\")\n",
    "        analysis = analyze_kernel_with_gpt4(\n",
    "            kernel_code=kernel_code,\n",
    "            gpu_model=gpu_model,\n",
    "            grid_dim=scaled_grid,\n",
    "            block_dim=base_block_dim,\n",
    "            data_size=data_size\n",
    "        )\n",
    "        \n",
    "        if 'error' not in analysis:\n",
    "            timing = analysis.get('execution_time_estimate', {})\n",
    "            results.append({\n",
    "                'data_size': data_size,\n",
    "                'execution_time': timing.get('microseconds_typical', 0),\n",
    "                'grid_dim': scaled_grid,\n",
    "                'analysis': analysis\n",
    "            })\n",
    "        else:\n",
    "            print(f\"  Error for size {data_size}: {analysis['error']}\")\n",
    "    \n",
    "    if results:\n",
    "        # Plot scaling behavior\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        data_sizes = [r['data_size'] for r in results]\n",
    "        times = [r['execution_time'] for r in results]\n",
    "        \n",
    "        plt.loglog(data_sizes, times, 'o-', linewidth=2, markersize=8)\n",
    "        plt.xlabel('Data Size (elements)')\n",
    "        plt.ylabel('Execution Time (Œºs)')\n",
    "        plt.title('Scaling Behavior (Log-Log Plot)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        throughput = [ds / (t * 1e-6) for ds, t in zip(data_sizes, times)]  # elements/second\n",
    "        plt.semilogx(data_sizes, throughput, 's-', linewidth=2, markersize=8, color='orange')\n",
    "        plt.xlabel('Data Size (elements)')\n",
    "        plt.ylabel('Throughput (elements/sec)')\n",
    "        plt.title('Throughput vs Data Size')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print scaling summary\n",
    "        print(\"\\nüìà SCALING ANALYSIS SUMMARY:\")\n",
    "        print(f\"Data Size Range: {min(data_sizes):,} - {max(data_sizes):,} elements\")\n",
    "        print(f\"Time Range: {min(times):.2f} - {max(times):.2f} Œºs\")\n",
    "        print(f\"Peak Throughput: {max(throughput):.2e} elements/sec\")\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        print(\"No successful analyses to plot\")\n",
    "        return []\n",
    "\n",
    "def export_analysis_report(analysis: Dict, filename: str = None):\n",
    "    \"\"\"\n",
    "    Export analysis results to a formatted text report.\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"kernel_analysis_report_{timestamp}.txt\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"CUDA KERNEL PERFORMANCE ANALYSIS REPORT\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        # Metadata\n",
    "        metadata = analysis.get('_metadata', {})\n",
    "        f.write(f\"Analysis Date: {metadata.get('timestamp', 'Unknown')}\\n\")\n",
    "        f.write(f\"Target GPU: {metadata.get('gpu_target', 'Unknown')}\\n\")\n",
    "        f.write(f\"Grid Dimensions: {metadata.get('grid_dim', 'Unknown')}\\n\")\n",
    "        f.write(f\"Block Dimensions: {metadata.get('block_dim', 'Unknown')}\\n\")\n",
    "        f.write(f\"Data Size: {metadata.get('data_size', 'Unknown')}\\n\\n\")\n",
    "        \n",
    "        # Execution time\n",
    "        timing = analysis.get('execution_time_estimate', {})\n",
    "        f.write(\"EXECUTION TIME ESTIMATE:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(f\"Minimum: {timing.get('microseconds_min', 'N/A')} Œºs\\n\")\n",
    "        f.write(f\"Typical: {timing.get('microseconds_typical', 'N/A')} Œºs\\n\")\n",
    "        f.write(f\"Maximum: {timing.get('microseconds_max', 'N/A')} Œºs\\n\")\n",
    "        f.write(f\"Confidence: {timing.get('confidence_level', 'N/A')}\\n\\n\")\n",
    "        \n",
    "        # Performance analysis\n",
    "        perf = analysis.get('performance_analysis', {})\n",
    "        f.write(\"PERFORMANCE ANALYSIS:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(f\"Primary Bottleneck: {perf.get('primary_bottleneck', 'N/A')}\\n\")\n",
    "        f.write(f\"Memory Pattern: {perf.get('memory_pattern', 'N/A')}\\n\")\n",
    "        f.write(f\"Branch Divergence Risk: {perf.get('branch_divergence_risk', 'N/A')}\\n\")\n",
    "        f.write(f\"Occupancy Estimate: {perf.get('occupancy_estimate', 'N/A')}\\n\")\n",
    "        f.write(f\"Arithmetic Intensity: {perf.get('arithmetic_intensity', 'N/A')} ops/byte\\n\\n\")\n",
    "        \n",
    "        if perf.get('bottleneck_explanation'):\n",
    "            f.write(f\"Bottleneck Explanation:\\n{perf['bottleneck_explanation']}\\n\\n\")\n",
    "        \n",
    "        # Resource utilization\n",
    "        resources = analysis.get('resource_utilization', {})\n",
    "        f.write(\"RESOURCE UTILIZATION:\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(f\"Registers per Thread: {resources.get('registers_per_thread', 'N/A')}\\n\")\n",
    "        f.write(f\"Shared Memory per Block: {resources.get('shared_memory_per_block_bytes', 'N/A')} bytes\\n\")\n",
    "        f.write(f\"Global Memory Transactions: {resources.get('global_memory_transactions', 'N/A')}\\n\")\n",
    "        f.write(f\"Instruction Count Estimate: {resources.get('instruction_count_estimate', 'N/A')}\\n\\n\")\n",
    "        \n",
    "        # Optimization suggestions\n",
    "        optimizations = analysis.get('optimization_suggestions', [])\n",
    "        if optimizations:\n",
    "            f.write(\"OPTIMIZATION SUGGESTIONS:\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for i, opt in enumerate(optimizations, 1):\n",
    "                f.write(f\"{i}. [{opt.get('category', 'general').upper()}] {opt.get('suggestion', 'N/A')}\\n\")\n",
    "                f.write(f\"   Expected Improvement: {opt.get('expected_improvement', 'N/A')}\\n\")\n",
    "                f.write(f\"   Difficulty: {opt.get('difficulty', 'N/A')}\\n\\n\")\n",
    "        \n",
    "        # Scaling analysis\n",
    "        scaling = analysis.get('scaling_analysis', {})\n",
    "        if scaling:\n",
    "            f.write(\"SCALING ANALYSIS:\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Data Size Scaling: {scaling.get('scales_with_data_size', 'N/A')}\\n\")\n",
    "            f.write(f\"Parallel Efficiency: {scaling.get('parallel_efficiency', 'N/A')}\\n\")\n",
    "            f.write(f\"Weak Scaling: {scaling.get('weak_scaling_behavior', 'N/A')}\\n\")\n",
    "            f.write(f\"Strong Scaling: {scaling.get('strong_scaling_behavior', 'N/A')}\\n\\n\")\n",
    "        \n",
    "        # Additional notes\n",
    "        if analysis.get('comparison_notes'):\n",
    "            f.write(\"ADDITIONAL NOTES:\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"{analysis['comparison_notes']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"Report generated by GPU Kernel Time Predictor\\n\")\n",
    "    \n",
    "    print(f\"üìÑ Analysis report exported to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "print(\"Advanced analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup Notes\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "1. **OpenAI API Key for Google Colab**:\n",
    "   - Click the **key icon** (üîë) in the left sidebar of Colab\n",
    "   - Click **\"Add new secret\"**\n",
    "   - Name: `OPENAI_API_KEY`\n",
    "   - Value: Your OpenAI API key\n",
    "   - Toggle notebook access to ON\n",
    "   \n",
    "   *Alternative for local Jupyter*: Set environment variable `OPENAI_API_KEY=your_api_key_here`\n",
    "\n",
    "2. **GPU Models**: The notebook includes specifications for common GPUs used in research. You can add more GPU models by extending the `GPU_SPECS` dictionary.\n",
    "\n",
    "3. **Cost Considerations**: Each analysis call costs approximately $0.01-0.05 depending on kernel complexity and response length.\n",
    "\n",
    "### Usage Tips\n",
    "\n",
    "- **Kernel Complexity**: More complex kernels will get more detailed and accurate analysis\n",
    "- **Launch Configuration**: Providing realistic grid/block dimensions improves accuracy\n",
    "- **Data Size**: Including expected data sizes helps with memory bandwidth calculations\n",
    "- **Multiple Analyses**: Use batch analysis for comparing optimization variants\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Estimates Only**: GPT-4 provides educated estimates, not precise measurements\n",
    "- **Architecture Specifics**: Analysis quality depends on how well GPT-4 knows your target GPU\n",
    "- **Dynamic Behavior**: Cannot predict runtime-dependent behavior like dynamic branching\n",
    "- **Validation Needed**: Always validate predictions with actual profiling (nvprof, nsys, ncu)\n",
    "\n",
    "This notebook is designed to complement your CUDA microbenchmarking work by providing quick performance insights before implementation and helping identify potential optimizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
